{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS DATA FOR HUMAN OBSERVED DATASET ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processdata(dataset):\n",
    "    \n",
    "    imagename=dataset['img_id'].values\n",
    "    features=dataset[['f1','f2','f3','f4','f5','f6','f7','f8','f9']].values\n",
    "    \n",
    "    return imagename,features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSING DATA FOR GSC DATASET ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processdataGSC(dataset):\n",
    "    \n",
    "    imagename=(dataset[:,0])\n",
    "   \n",
    "    features=dataset[:,1:dataset.shape[1]]\n",
    "    \n",
    "    return imagename, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pair_features(nptrainingpairs,imagename,features):\n",
    "    \n",
    "    \n",
    "    feature1=np.asarray([[0.0 for i in range(0,features.shape[1])] for j in range(0,nptrainingpairs.shape[0])])\n",
    "    feature2=np.asarray([[0.0 for i in range(0,features.shape[1])] for j in range(0,nptrainingpairs.shape[0])])\n",
    "\n",
    "    for i in range (0,nptrainingpairs.shape[0]):\n",
    "        img1=nptrainingpairs[i][0]\n",
    "\n",
    "        for j in range (0,imagename.shape[0]):\n",
    "            if(img1 == imagename[j]):\n",
    "                feature1[i]=features[j]\n",
    "               \n",
    "    \n",
    "    for i in range (0,nptrainingpairs.shape[0]):\n",
    "        img2=nptrainingpairs[i][1]\n",
    "      \n",
    "        for j in range (0,imagename.shape[0]):\n",
    "            if img2==imagename[j]:\n",
    "                feature2[i]=features[j]           \n",
    "    \n",
    "    return (feature1,feature2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract(first,second):\n",
    "    rows=first.shape[0]\n",
    "    cols=first.shape[1]\n",
    "    difference=np.asarray([[0.0 for i in range(0,cols)] for j in range(0,rows)])\n",
    "    \n",
    "    for i in range (0,rows):\n",
    "        for j in range (0,cols):\n",
    "            difference[i][j]=first[i][j]-second[i][j]\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(first, second):\n",
    "    concat=np.concatenate((first,second),axis=1)\n",
    "    print(concat.shape)\n",
    "    return concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARING HUMAN OBSERVED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 3)\n"
     ]
    }
   ],
   "source": [
    "    trainingdata1= pd.read_csv(\"HumanObserved-Features-Data.csv\")\n",
    "    imagenames, features=processdata(trainingdata1)\n",
    "    \n",
    "    \n",
    "    trainingpairs=np.array(pd.read_csv(\"same_pairs.csv\").values)\n",
    "    different_train_pairs=np.array(pd.read_csv(\"diffn_pairs.csv\").values)\n",
    "    \n",
    "    \n",
    "    #deleting extra pairs from different pair dataset\n",
    "    different_train_pairs=np.delete(different_train_pairs,np.s_[trainingpairs.shape[0]:],axis=0)\n",
    "    #creating training dataset with equal same and different pairs\n",
    "    pairs=np.concatenate((trainingpairs,different_train_pairs),axis=0)\n",
    "    #shuffling the dataset\n",
    "    np.random.shuffle(pairs)\n",
    "    print(pairs.shape)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARING GSC DATASET "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE FROM COMMENT IF YOU WANNA RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#     trainingdata1= pd.read_csv(\"GSC-Features.csv\")\n",
    "#     trainingdata1=np.array(trainingdata1)\n",
    "#     #print(trainingdata1.shape)\n",
    "#     imagenames, features=processdataGSC(trainingdata1)\n",
    "    \n",
    "#     trainingpairs=np.array(pd.read_csv(\"same_pairs_gsc.csv\").values)  #71531 rows\n",
    "#     different_train_pairs=np.array(pd.read_csv(\"diffn_pairs_gsc.csv\").values) #7622557 rows\n",
    "#     trainingpairs=trainingpairs[0:1000]\n",
    "# #     print(\"same pair\",trainingpairs.shape)\n",
    "#     #deleting extra pairs from different pair dataset\n",
    "#     different_train_pairs=np.delete(different_train_pairs,np.s_[trainingpairs.shape[0]:],axis=0)\n",
    "#     #creating training dataset with equal same and different pairs\n",
    "#     pairs=np.concatenate((trainingpairs,different_train_pairs),axis=0)\n",
    "#     #shuffling the dataset\n",
    "#     np.random.shuffle(pairs)\n",
    "#     print(pairs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 1. 0. 2. 2. 0. 2. 2.]\n",
      "[1. 1. 1. 1. 2. 1. 0. 0. 2.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature1, feature2 = extract_pair_features(pairs,imagenames,features)\n",
    "#-------------------checks to ensure correct evaluation--------------------------#\n",
    "print(feature1[0])\n",
    "print(feature2[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PREPARING TRAINING VALIDATION AND TESTING SETS FOR DATA WITH SUBTRACTED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 9)\n",
      "[ 1.  0.  0. -1.  0.  1.  0.  2.  0.]\n"
     ]
    }
   ],
   "source": [
    "    #CREATING SUBTRACTED FEATURE DATASET\n",
    "    subtracted_features=subtract(feature1,feature2)\n",
    "#-------------------checks to ensure correct evaluation--------------------------#    \n",
    "    print(subtracted_features.shape)\n",
    "    print(subtracted_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape (1265,)\n",
      "validation shape (158,)\n",
      "testing shape (158,)\n"
     ]
    }
   ],
   "source": [
    "#PREPARING TRAINING VALIDATION AND TESTING DATASETS WITH SUBTRACTED FEATURES\n",
    "\n",
    "trainingpercent=0.8\n",
    "validationpercent=0.1\n",
    "testingpercent=0.1\n",
    "\n",
    "sub_Training_x=(subtracted_features[0:int((len(subtracted_features))*trainingpercent)])\n",
    "sub_Training_t=pairs[0:int((len(pairs))*trainingpercent),2]\n",
    "\n",
    "\n",
    "sub_Validation_x=subtracted_features[int((len(subtracted_features))*trainingpercent):int((len(subtracted_features))*trainingpercent)+int(len(subtracted_features)*validationpercent)]\n",
    "sub_Validation_t=pairs[int((len(subtracted_features))*trainingpercent):int((len(subtracted_features))*trainingpercent)+int(len(subtracted_features)*validationpercent),2]\n",
    "\n",
    "\n",
    "sub_Test_x=subtracted_features[int((len(subtracted_features))*trainingpercent)+int(len(subtracted_features)*validationpercent):int((len(subtracted_features))*trainingpercent)+int(len(subtracted_features)*validationpercent)+int(len(subtracted_features)*testingpercent)]\n",
    "sub_Test_t=pairs[int((len(subtracted_features))*trainingpercent)+int(len(subtracted_features)*validationpercent):int((len(subtracted_features))*trainingpercent)+int(len(subtracted_features)*validationpercent)+int(len(subtracted_features)*testingpercent),2]\n",
    "\n",
    "print(\"training shape\",sub_Training_t.shape)\n",
    "print(\"validation shape\",sub_Validation_t.shape)\n",
    "print(\"testing shape\", sub_Test_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARING TRAINING VALIDATION AND TESTING SETS FOR DATA WITH CONCATENATED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 18)\n",
      "[[2. 1. 1. ... 0. 0. 2.]\n",
      " [2. 1. 1. ... 0. 3. 2.]\n",
      " [2. 1. 1. ... 0. 3. 2.]\n",
      " ...\n",
      " [2. 1. 1. ... 0. 1. 1.]\n",
      " [2. 1. 1. ... 1. 1. 2.]\n",
      " [2. 1. 1. ... 0. 3. 2.]]\n"
     ]
    }
   ],
   "source": [
    "#CREATING CONCATENATED FEATURES DATASET\n",
    "concat_features=concat(feature1,feature2)\n",
    "print(concat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING, VALIDATION AND TESTING DATA FOR CONCATENATED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1265, 18)\n",
      "(158, 18)\n",
      "(158, 18)\n"
     ]
    }
   ],
   "source": [
    "#PREPARING TRAINING VALIDATION AND TESTING DATASETS WITH CONCATENATED FEATURES\n",
    "\n",
    "\n",
    "trainingpercent=0.8\n",
    "validationpercent=0.1\n",
    "testingpercent=0.1\n",
    "\n",
    "\n",
    "concat_Training_x=(concat_features[0:int((len(concat_features))*trainingpercent)])\n",
    "concat_Training_t=pairs[0:int((len(pairs))*trainingpercent),2]\n",
    "\n",
    "\n",
    "concat_Validation_x=concat_features[int((len(concat_features))*trainingpercent):int((len(concat_features))*trainingpercent)+int(len(concat_features)*validationpercent)]\n",
    "concat_Validation_t=pairs[int((len(concat_features))*trainingpercent):int((len(concat_features))*trainingpercent)+int(len(concat_features)*validationpercent),2]\n",
    "\n",
    "\n",
    "concat_Test_x=concat_features[int((len(concat_features))*trainingpercent)+int(len(concat_features)*validationpercent):int((len(concat_features))*trainingpercent)+int(len(concat_features)*validationpercent)+int(len(concat_features)*testingpercent)]\n",
    "concat_Test_t=pairs[int((len(concat_features))*trainingpercent)+int(len(concat_features)*validationpercent):int((len(concat_features))*trainingpercent)+int(len(concat_features)*validationpercent)+int(len(concat_features)*testingpercent),2]\n",
    "\n",
    "print(concat_Training_x.shape)\n",
    "print(concat_Validation_x.shape)\n",
    "print(concat_Test_x.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return (1.0 / (1 + np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(training_data, calculated_y, expected_y):\n",
    "        delta=np.dot(training_data,(calculated_y-expected_y))\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(x,a,t,epochs,w,l_rate):\n",
    "    for epoch in range(epochs):\n",
    "        a = np.array(sigmoid(x.dot(w.T)))\n",
    "        w = w - (l_rate*((a-t.T).dot(x)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(calc_output, target):\n",
    "    counter=0\n",
    "    for i in range(0,target.shape[0]):\n",
    "        if (round(calc_output[i])==target[i]):\n",
    "            counter=counter+1\n",
    "    return(counter/(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION FOR SUBTRACTED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!***************SUBTRACTED FEATURES******************!!\n",
      "%================logistic regression===================\n",
      "||||||||||||||||   VALIDATION SET   ||||||||||||||\n",
      "NUMBER OF EPOCHS 150\n",
      "Learning Rate  0.05\n",
      "Accuracy\n",
      "0.6772151898734177\n",
      "%================logistic regression===================\n",
      "||||||||||||||||   TESTING SET   ||||||||||||||\n",
      "NUMBER OF EPOCHS 150\n",
      "Learning Rate  0.05\n",
      "Accuracy\n",
      "0.6075949367088608\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#INITIALIZING THE WEIGHTS, epochs and learning rate\n",
    "Epochs=150\n",
    "Learning_rate=0.05\n",
    "# w=np.random.rand(512) #initial weights for GSC dataset\n",
    "w=np.array([0.81130218, 0.1011935,  0.8209283,  0.87660455, 0.58273304, 0.57651164, 0.07282913, 0.08728755, 0.62051645])\n",
    "# print(\"initial weights are \",w)\n",
    "#print(\"shape of weights is \",w.shape)\n",
    "#print(subtracted_features.shape)\n",
    "\n",
    "\n",
    "for i in range(0,Epochs):\n",
    "\n",
    "    wtx= np.dot(w,np.transpose(sub_Training_x))\n",
    "    wtx = wtx.astype(float)\n",
    "    a=sigmoid(wtx)\n",
    "    deltaw=derivative(np.transpose(sub_Training_x),a,sub_Training_t)\n",
    "    w=w-Learning_rate*deltaw\n",
    "    #print(i,\"th set of updated weights are as follows\",w)\n",
    "        \n",
    "        \n",
    "v=np.dot(w,np.transpose(sub_Validation_x))\n",
    "v=v.astype(float)\n",
    "final=sigmoid(v)\n",
    "\n",
    "\n",
    "#print(final.shape)\n",
    "print(\"!!***************SUBTRACTED FEATURES******************!!\")\n",
    "print(\"%================logistic regression===================\")\n",
    "print(\"||||||||||||||||   VALIDATION SET   ||||||||||||||\")\n",
    "print(\"NUMBER OF EPOCHS\",Epochs,)\n",
    "print(\"Learning Rate \", Learning_rate)\n",
    "print(\"Accuracy\")\n",
    "print(Accuracy(final,sub_Validation_t))\n",
    "print(\"%================logistic regression===================\")\n",
    "print(\"||||||||||||||||   TESTING SET   ||||||||||||||\")\n",
    "t=np.dot(w,np.transpose(sub_Test_x))\n",
    "t=t.astype(float)\n",
    "finalt=sigmoid(t)\n",
    "\n",
    "print(\"NUMBER OF EPOCHS\",Epochs,)\n",
    "print(\"Learning Rate \", Learning_rate)\n",
    "print(\"Accuracy\")\n",
    "print(Accuracy(finalt,sub_Test_t))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION FOR CONCATENATED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!**************CONCATENATED FEATURES****************!!\n",
      "%================logistic regression===================\n",
      "||||||||||||||||   VALIDATION SET   ||||||||||||||\n",
      "NUMBER OF EPOCHS 1000\n",
      "Learning Rate  0.05\n",
      "Accuracy\n",
      "0.9493670886075949\n",
      "%================logistic regression===================\n",
      "||||||||||||||||   TESTING SET   ||||||||||||||\n",
      "NUMBER OF EPOCHS 1000\n",
      "Learning Rate  0.05\n",
      "Accuracy\n",
      "0.9050632911392406\n"
     ]
    }
   ],
   "source": [
    "# w_concat=np.random.rand(1024)# initial weights for GSC dataset\n",
    "w_concat=np.array([0.81130218, 0.1011935,  0.8209283,  0.87660455, 0.58273304, 0.57651164,0.6475638,\n",
    "                   0.07282913, 0.08728755, 0.62051645,0.653624,0.213523532,0.356436,0.984264,0.12454,0,0.1234567,0.438563])\n",
    "#print(w_concat)\n",
    "concat_Epochs=1000\n",
    "Learning_rate_concat=0.05\n",
    "\n",
    "\n",
    "for i in range(0,concat_Epochs):\n",
    "\n",
    "    wtx_concat= np.dot(w_concat,np.transpose(concat_Training_x))\n",
    "    wtx_concat = wtx_concat.astype(float)\n",
    "    a_concat=sigmoid(wtx_concat)\n",
    "    deltaw_concat=derivative(np.transpose(concat_Training_x),a_concat,concat_Training_t)\n",
    "    w_concat=w_concat-Learning_rate_concat*deltaw_concat\n",
    "\n",
    "v=np.dot(w_concat,np.transpose(concat_Validation_x))\n",
    "v=v.astype(float)\n",
    "finalv=sigmoid(v)\n",
    "\n",
    "print(\"!!**************CONCATENATED FEATURES****************!!\")\n",
    "print(\"%================logistic regression===================\")\n",
    "print(\"||||||||||||||||   VALIDATION SET   ||||||||||||||\")\n",
    "print(\"NUMBER OF EPOCHS\",concat_Epochs)\n",
    "print(\"Learning Rate \", Learning_rate_concat)\n",
    "print(\"Accuracy\")\n",
    "print(Accuracy(finalv,concat_Validation_t))\n",
    "\n",
    "print(\"%================logistic regression===================\")\n",
    "print(\"||||||||||||||||   TESTING SET   ||||||||||||||\")\n",
    "testing=np.dot(w_concat,np.transpose(concat_Test_x))\n",
    "testing=testing.astype(float)\n",
    "finalt=sigmoid(testing)\n",
    "print(\"NUMBER OF EPOCHS\",concat_Epochs)\n",
    "print(\"Learning Rate \", Learning_rate_concat)\n",
    "print(\"Accuracy\")\n",
    "    \n",
    "print(Accuracy(finalt,concat_Test_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LINEAR REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(train_data,train_target,val_data,val_target,test_data,test_target):\n",
    "    M = 5\n",
    "    epochs =3200\n",
    "    learningRate = 0.001\n",
    "    kmeans = KMeans(n_clusters=M, random_state=0).fit(train_data)\n",
    "    Mu = np.array(kmeans.cluster_centers_)\n",
    "\n",
    "    #print(kmeans,Mu)\n",
    "    train_target = np.array(train_target)\n",
    "    BigSigma = GenerateBigSigma(train_data,Mu)\n",
    "    training_PHI = GetPhiMatrix(train_data,Mu,BigSigma)\n",
    "#     print(training_PHI,training_PHI.shape)    \n",
    "    \n",
    "    Wts = np.random.rand(training_PHI.shape[1])\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    for i in range(0,epochs):\n",
    "    \n",
    "        wTx = np.array(np.dot(Wts,training_PHI.T))\n",
    "#         print(\"wtx\",wTx.shape)\n",
    "        Delta_E_D = -np.dot((np.subtract(train_target.T,wTx)),training_PHI)\n",
    "#         print(\"deled\",Delta_E_D.shape)\n",
    "        Wts = Wts - np.dot(learningRate,Delta_E_D)\n",
    "#         print(Wts.shape)\n",
    "\n",
    "    testing_PHI= GetPhiMatrix(test_data,Mu, BigSigma)\n",
    "    final= np.array(np.dot(Wts,testing_PHI.T))\n",
    "#     print(final)\n",
    "    acuuracy, erms = GetErms(final,test_target)\n",
    "    return acuuracy, epochs, learningRate,erms\n",
    "    \n",
    "    \n",
    "def GetErms(Output,Act_Target):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    Output = np.transpose(Output)\n",
    "    for i in range (len(Output)):\n",
    "        sum = sum + math.pow((Act_Target[i] - Output[i]),2)\n",
    "#         print(sum)\n",
    "        if(int(np.around(Output[i], 0)) == Act_Target[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(Output)))\n",
    "    Erms=(math.sqrt(sum/len(Output)))\n",
    "    return accuracy, Erms \n",
    "    \n",
    "def GenerateBigSigma(Data, MuMatrix):\n",
    "    #print(\"\\t\\t\",Data.shape, MuMatrix.shape)\n",
    "    BigSigma    = np.zeros((len(Data[1]),len(Data[1])))\n",
    "    #print(BigSigma.shape)\n",
    "    DataT       = np.transpose(Data)\n",
    "    Len = len(DataT)       \n",
    "    varVect     = []\n",
    "    for i in range(len(Data[0])):\n",
    "        vct = []\n",
    "        for j in range(Len):\n",
    "            vct.append(DataT[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    \n",
    "    for j in range(len(DataT)):\n",
    "        if varVect[j]==0:\n",
    "            varVect[j] = varVect[j] + 0.00001\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    #print (\"BigSigma Generated..\")\n",
    "    #print(BigSigma)\n",
    "    return BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    #print(DataRow.shape,MuRow.shape)\n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,R)  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma):\n",
    "    #print(Data.shape)\n",
    "    PHI = np.zeros((len(Data),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    #print(BigSigInv.shape)\n",
    "    #print(MuMatrix.shape)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,len(Data)):\n",
    "            #print(Data[R].shape,MuMatrix[C].shape,BigSigInv.shape)\n",
    "            PHI[R][C] = GetRadialBasisOut(Data[R], MuMatrix[C], BigSigInv)\n",
    "\n",
    "    #print (PHI)\n",
    "    return PHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calling linear regression method for subtracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!**************SUBTRACTED FEATURES****************!!\n",
      "%================LINEAR regression===================\n",
      "||||||||||||||||   TESTING SET   ||||||||||||||\n",
      "NUMBER OF EPOCHS 3200\n",
      "Learning Rate  0.001\n",
      "Erms  0.7208163273949116\n"
     ]
    }
   ],
   "source": [
    "accuracy,epoch,learningrate,erms=linear_regression(sub_Training_x, sub_Training_t, sub_Validation_x,sub_Validation_t,sub_Test_x,sub_Test_t)\n",
    "print(\"!!**************SUBTRACTED FEATURES****************!!\")\n",
    "print(\"%================LINEAR regression===================\")\n",
    "print(\"||||||||||||||||   TESTING SET   ||||||||||||||\")\n",
    "print(\"NUMBER OF EPOCHS\",epoch)\n",
    "print(\"Learning Rate \", learningrate)\n",
    "# print(\"Accuracy \", accuracy)\n",
    "print(\"Erms \", erms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calling linear regression method for concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!**************CONCATENATED FEATURES****************!!\n",
      "%================LINEAR regression===================\n",
      "||||||||||||||||   TESTING SET   ||||||||||||||\n",
      "NUMBER OF EPOCHS 3200\n",
      "Learning Rate  0.001\n",
      "Erms   0.7334675749781334\n"
     ]
    }
   ],
   "source": [
    "accuracy,epoch,learningrate,erms=linear_regression(concat_Training_x, concat_Training_t, concat_Validation_x,concat_Validation_t,concat_Test_x,concat_Test_t)\n",
    "print(\"!!**************CONCATENATED FEATURES****************!!\")\n",
    "print(\"%================LINEAR regression===================\")\n",
    "print(\"||||||||||||||||   TESTING SET   ||||||||||||||\")\n",
    "print(\"NUMBER OF EPOCHS\",epoch)\n",
    "print(\"Learning Rate \", learningrate)\n",
    "# print(\"Accuracy\", accuracy)\n",
    "print(\"Erms  \", erms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(sub_Training_x.shape)\n",
    "input_size = sub_Training_x.shape[1]\n",
    "drop_out = 0.15\n",
    "first_dense_layer_nodes  = 256\n",
    "second_dense_layer_nodes = 128\n",
    "third_dense_layer_nodes = 2\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(drop_out))\n",
    "\n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    model.add(Dense(third_dense_layer_nodes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    opt = Adam()\n",
    "\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUNNING THE TRAINING DATA ON THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_split = 0.2\n",
    "num_epochs = 10000\n",
    "model_batch_size = 128\n",
    "tb_batch_size = 32\n",
    "early_patience = 100\n",
    "\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "# Read Dataset\n",
    "# dataset, target = get_feature_matrix(data='hod', method='concatenate')\n",
    "\n",
    "y_train = sub_Training_t\n",
    "X_train   = sub_Training_x\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "\n",
    "y_train = encodeLabel(y_train)\n",
    "\n",
    "\n",
    "# Process Dataset\n",
    "# processedData, processedLabel = processData(dataset)\n",
    "history = model.fit(X_train\n",
    "                    , y_train\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeLabel(encodedLabel):\n",
    "    if encodedLabel == 0:\n",
    "        return 0\n",
    "    elif encodedLabel == 1:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING ACCURACY for subtracted data human observed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "y_test = np.array(sub_Test_x) #if we un comment the part where GSC data is being processed into sub_test and concat test we will get accuracy for both those datasets \n",
    "X_test = np.transpose(sub_Test_t)\n",
    "\n",
    "\n",
    "processedTestData  = X_test\n",
    "processedTestLabel = encodeLabel(y_test)\n",
    "predictedTestLabel = []\n",
    "\n",
    "for i,j in zip(processedTestData,processedTestLabel):\n",
    "    y = model.predict(np.array(i).reshape(-1,processedTestData.shape[1]))\n",
    "    predictedTestLabel.append(decodeLabel(y.argmax()))\n",
    "    \n",
    "    if j.argmax() == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: {0:.2f}%\".format(right/(right+wrong)*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING ACCURACY for concatenated data human observed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "y_test = np.array(concat_Test_t)\n",
    "X_test = np.transpose(concat_Test_x)\n",
    "\n",
    "\n",
    "processedTestData  = X_test\n",
    "processedTestLabel = encodeLabel(y_test)\n",
    "predictedTestLabel = []\n",
    "\n",
    "for i,j in zip(processedTestData,processedTestLabel):\n",
    "    y = model.predict(np.array(i).reshape(-1,processedTestData.shape[1]))\n",
    "    predictedTestLabel.append(decodeLabel(y.argmax()))\n",
    "    \n",
    "    if j.argmax() == y.argmax():\n",
    "        right = right + 1\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "\n",
    "print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\"Testing Accuracy: {0:.2f}%\".format(right/(right+wrong)*100))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
